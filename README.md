# ğŸ§  Sign Language Binary Classification (NumPy â€“ From Scratch)

## ğŸ“Œ Overview
This project implements a simple neural network from scratch using only **NumPy**, with no use of high-level frameworks like TensorFlow or PyTorch. The task is a **binary classification** of hand-written digits (`4` vs. `5`) from the Sign Language Digits Dataset.

This work was submitted as **Part 1** of the mid-semester milestone in the *Basics of Deep Learning* course.

---

## ğŸ“‚ Files
```
sign-language-numpy/
â”œâ”€â”€ notebook.ipynb      # Main notebook with all code
â”œâ”€â”€ report.pdf           # Final project report
â”œâ”€â”€ README.md                                # Project description
â”œâ”€â”€ requirements.txt                         # Python dependencies
```

---

## ğŸ“Š Dataset
- **Source:** Sign Language Digits Dataset
- **Classes Used:** Only digits `4` and `5` (binary classification)
- **Images:** 28x28 grayscale, flattened into 784-dim vectors
- **Preprocessing:**
  - Normalized pixel values to [0, 1]
  - Filtered only digits 4 and 5
  - Labeled: `4 â†’ 0`, `5 â†’ 1`
  - Train/Validation/Test split: 80% / 0% / 20%

---

## ğŸ—ï¸ Model Architecture
- Input Layer: 784 neurons (flattened image)
- Hidden Layer: 32 neurons, **Sigmoid** activation
- Output Layer: 1 neuron, **Sigmoid** activation

### âš™ï¸ Training Details
- **Loss Function:** Binary Cross-Entropy
- **Optimizer:** Gradient Descent (custom implementation)
- **Learning Rate:** 0.01
- **Epochs:** 50
- **Framework:** Implemented fully using NumPy

---

## ğŸ“ˆ Evaluation & Results
- Accuracy: **97.5%** (on test set)
- Confusion Matrix:
  - TP = 94, TN = 101, FP = 2, FN = 3
- Visualized:
  - Loss curve across epochs
  - Example image + model prediction with confidence (0.9996)

---

## ğŸš€ How to Run
1. Open the notebook in **Google Colab**
2. Ensure the dataset is accessible via the provided link or manually upload
3. Run all cells in order:
   - Preprocessing
   - Training (forward + backward propagation)
   - Evaluation (metrics + plots)

---

## ğŸ“ Key Concepts Implemented
- Forward propagation
- Backpropagation
- Sigmoid activation
- Cross-entropy loss
- Manual weight updates with gradient descent
- Accuracy & confusion matrix calculation (no external libraries)

---

## ğŸ‘¥ Authors
- **Shir Zohar**
- **Sivan Zagdon**

---

## ğŸ“œ License
This project was developed for educational purposes at Colman College, as part of the "Basics of Deep Learning" course.

